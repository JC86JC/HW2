---
title: "HW2 STA521"
author: 'Jiawei Chen jc762 JC86JC'
date: "Due September 15, 2019 5pm"
output:
  pdf_document:
    fig_caption: yes
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Exploratory Data Analysis


```{r data, message = FALSE, warning = FALSE, echo = FALSE}
library(alr3)
data(UN3, package="alr3")
library(car)
library(GGally)
library(dplyr)
library(knitr)
```


#### 1. Create a summary of the data.

The summary of the data is shown below. Judging by the no. of NA's in the output, `ModernC`, `Change`, `PPgdp`, `Frate`, `Pop` and `Fertility` in total 6 variables have missing data. Among them, the data-missing problem is particularly serious for `ModernC` and `Frate`, which have 58 and 43 missing values out of 210 respectively. We can also know from the data description that only 125 observations have data present for all variables.  

Based on variable descriptions and their structures, all of these variables should be considered as quantitative.  

Besides, it's worth noticing that other than `Change`, all variables are strictly greater than 0.

```{r}
summary(UN3)
UN3 = na.omit(UN3)
```

\pagebreak

#### 2. Investigate the predictors graphically.

$~$
The following **Figure 1** shows the scatter plots between the variables. First of all, from the density plots on the diagonal, we can see that the distributions of `PPgdp` and `Pop` are particularly right skewed, resulting in difficulties in model fitting and interpretations. Besides, the range of these variables is also large. Thus, we should consider transfromations on these two predictors. 

On the other hand, the range for `Change` is rather small, indicating that transformation on it might be of little use. Also, it includes negative values, which means it needs to be pre-transformed into positive variable before implementing power transformations.

In addition, from the first column it seems that `ModernC` has linear relationship with `Change`, `Fertility` and `Purban`, and non-linear relationship with `PPgdp` and `Frate`. Also, it's indicated that most predictors seem to have relationships with each other, and the ones between `Fertility` & `Change` and `Fertility` & `Purban` appear to be linear; the relationships between other variables appear to be non-linear. 

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=10, fig.cap = "scatter plot for the UN3 data, untransformed"}
ggpairs(UN3) + theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

\pagebreak

In the following plot we try to take logs on `Pop` and `PPgdp` to get rid of their large ranges and to inspect pairwise relationships once again. This time, the relationships between `logPPgdp` and many other vairables including `ModernC` seem to be linear, whereas `logPop` still shows only weak non-linear relationships with the others. If we are to predict `ModernC` from the other variables, formal tests are needed to decide whether to transfrom or drop these predictors.

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=10, fig.cap= "scatter plot for the UN3 data, take logs on Pop and PPgdp"}
UN0 = UN3 %>%
  mutate(logPop = log(Pop), logPPgdp = log(PPgdp))
ggpairs(UN0, columns = c(1,2,9,4,8,6,7)) 
```
$~$
$~$


## Model Fitting

#### 3.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response.

The summary of the fitted model is shown in the R output below. As we can see, the coefficients for `Change`, `PPgdp`, `Pop`, `Fertility` and the `intercept` seem to be significant with small P-values. The large P-values of coefficients for `Frate` and `Purban` indicates that these two terms are not significantly different from 0. The R-squared is 0.6183, indicating that about 61.8% of the variation is explained by our model. From the previous parts, we know that 85 observations are deleted due to missing data, so in total we've used 125 observations in fitting the model. This could also be known from the fact that the df for residual standard error is 118 and we are fitting 7 coefficients, which means 125 observations are used.

```{r}
fmodel = lm(ModernC ~ ., data = UN3)
summary(fmodel)
```

The following plot gives the diagnostic plots for our linear model. The Residuals vs Fitted plot shows no obvious patterns of the residuals, but may imply that unequal variance of residuals exists. From the Normal Q-Q plot, there are departures from normality of the standardized residuals, particularly on the upper end, but is not sufficient to render our assumption invalid. The Scale-location plot shows some signs of heteroscedasticity, since the residuals seem to spread the widest when fitted values is around 55, as is indicated by the hump of the red line. The Residuals vs Leverage plot shows no influential point in our model.

```{r fig.width=15, fig.height=12, fig.cap = "diagnostic plots"}
par(mfrow = c(2,2))
plot(fmodel)

```
$~$
$~$


#### 4. Examine added variable plots for your model above. 
$~$
The following figure  shows the added variable plots for the fitted model. Basically, they indicate the relationship between the response variable `ModernC` and the plotted predictor, after getting rid of the influence from the other predictors.   
Among the six plots, the most unusual one is the added-variable plot for `Pop` predictor. While the majority of the points crowd on the left side of the plot, two observations with row-names `China` and `India` is far away from them on the right. This, once again, proves that `Pop` predictor is particularly in need of transformation due to its overwhelminly strong right skewness. Also, the two observations mentioned, `China` and `India`, can be considered influential for `Pop` predictor.  
Apart from that, the plot for `PPgdp` also indicates some need of transfromation, and oberservations `Switzerland` and `Norway` are a bit influential to the fitted line in the plot for `PPgdp` predictor.


```{r fig.width=10, fig.height=10, fig.cap = 'added variable plot'}
avPlots(lm(ModernC ~., data = UN3))
```
$~$
$~$

#### 5. Using the multivariate BoxCox to find appropriate transformations of the response and predictor variables for the linear model.
$~$
In order to find the suitable transformations of our variables, we implement the multivariate BoxCox power transformation on all the variables of our linear model. Notice that since `Change` is not strictly positive, we decide to first transform it into `exp(Change)` to ensure positiveness.  
$~$
From the summary shown below, we can see that the suggested $\lambda$ = (1, 0, -0.16, 1, 0, 0, 1) for variables `ModernC`, `exp(Change)`, `PPgdp`, `Frate`, `Pop`, `Fertility` and `Purban` respectively. For convenience purpose, we just use $\lambda = 0$ for `PPgdp` power transformation.

Then we try to test whether the suggested $\lambda$ are actually nice powers by using a likelihood ratio test. The resulted P-value = 0.02, which is actually not a satisfactory result, but it doesn't mean the transformation is useless. For now we just stick with this transformation and see later how our model behaves.

```{r}
UN = UN3 %>%
  mutate(Change = exp(Change))
mul_transform = car::powerTransform(UN, family = 'bcPower')
summary(mul_transform, test)$result[,2]
```


```{r}
testTransform(mul_transform,lambda = c(1,0,0,1,0,0,1))
```
$~$
$~$

#### 6. Given the selected transformations of the predictors, verify the transformation of the response using `boxCox` and justify.
$~$
To verify the transformation acquired in the previous question, in this exercise we use `boxCox` function to transform the response variable using the transformed predictors.  
The first plot in the following figure shows the result of the transformation. As is shown, $\lambda = 1$ is well within the 95% confidence interval of the estimated $\lambda$ that maximises the log-likelihood.
If we apply the function to `ModernC` without transformed predictors, we will get the second plot, from which $\lambda = 1$ is hardly included in the 95% CI. The difference in these two results is caused by the fact that the predictors are not yet tranformed, so they are not at least weakly linearly related. 
```{r fig.width=8, fig.height=6, fig.cap = 'BoxCox transformations of ModernC'}
UN1 = UN %>%
  mutate(Change = log(Change), logPPgdp = log(PPgdp), 
         logPop = log(Pop), logFertility = log(Fertility), 
         PPgdp = NULL, Pop = NULL, Fertility = NULL)

model1 = lm(ModernC~., data = UN1)
boxCox(model1)
boxCox(fmodel)

```
$~$
$~$

#### 7.  Fit the regression using the transformed variables. 
$~$

We fit the model with our transformed data. This time, the coefficients of `Frate`, transformed `PPgdp`, transformed `Pop`, transformed `Fertility` and the intercept are significantly different from 0. However, the P-values for coefficients of `Change` and `Purban` and the intercept are large, indicating that these terms have little contributions to our model, and some further ANOVA tests could be considered to see whether to drop them or not.  

The figures below give the diagnostic plots and shows the added variable plots for this model. Similar to the original model, in the normal Q-Q plot there are some departures from the normality on both ends, but generally speaking the diagnostic plots show no obvious flaws of the model.  
In the added variable plots, the lines for `Change` and `Purban` are quite flat, indicating that they have no great contributions in the model. The good news is that no plots have indications of overly influential points this time.

```{r}
transmodel = lm(ModernC ~. , data = UN1)
summary(transmodel)
```


```{r fig.width=10, fig.height=8, fig.cap = 'diagnostic plots for the final model'}
par(mfrow = c(2,2))
plot(transmodel)
```


```{r fig.width=10, fig.height=10, fig.cap = 'added variable plots for the final model'}
avPlots(transmodel)
```
$~$
$~$

#### 8.  Are there any outliers or influential points in the data?
$~$
By carrying out the outlier tests and looking into the cook's distances, we can see that there are no outliers or overly influential points in the model.


```{r}
outlierTest(transmodel)
rownames(UN1)[cooks.distance(transmodel)>0.5]

```
$~$
$~$

## Summary of Results
$~$

#### 9. For your final model, provide summaries of coefficients.
$~$
Basically, the coefficients could be interpreted as the following: with all else held constant,

 - for every unit increase of annual population growth rate (`Change`), the percent of unmarried women using a modern method of contraception (`ModernC`) will increase by 2.31 units;

 - for every unit increase of percent of females over age 15 and economically active (`Frate`), `ModernC` will increase by 0.18 units;

 - for every unit increase of percent of population that is urban in 2001 (`Purban`), `ModernC` will decrease by 0.01 units;

 - for every 10% increase of per capita GDP in 2001 (`PPgdp`), `ModernC` will increase by $6.45*log(1.1) = 0.615$ units;

 - for every 10% increase of population in thousands (`Pop`), `ModernC` will increase by $1.6*log(1.1) = 0.152$ units;

 - for every 10% increase of expected number of live births per female in 2000 (`Fertility`), `ModernC` will decrease by $18.24*log(1.1) = 1.74$ units.

Also, for a country where there's no annual population growth rate, no females are over age 15 and economically active, 0 percent of population that is urban, per capita GDP is 1 USD in 2001, population is exactly 1000, and the expected number of live births per female is 1, the percent of unmarried women using a modern method of contraception is expected to be -15.11, which is in fact a figure that does not make sense.

The estimated coefficients and thier 95% CI's are shown in the table below.
```{r}
interval = confint(transmodel)
kable(cbind(transmodel$coefficients, interval), digits = 3,
      col.names = c('Estimate','2.5%','97.5%'))
```
$~$
$~$

#### 10. Provide a paragraph summarizing your final model and findings suitable for the US envoy to the UN.
$~$

Our final model is $ModernC = -15.11 + 2.31 Change + 0.18 Frate -0.01 Purban + 6.45 log(PPgdp) + 1.60 log(Pop) -18.24  log(Fertility) + \epsilon$ where the error $\epsilon\sim N(0,\sigma^2)$ is independent from all the other predictors.
$~$

As we can see from the model, the percent of unmarried women using a modern method of contraception is positively related to population growth rate and population of a locality. This can be considered as a good sign since it, to some extent, implies that populated areas are at least educating citizen to adopt proper modern method of contraception.
Besides, the positive relationship between `ModernC` and `PPgdp` is expected, since in most cases the more per capita gdp of a locality, the more educated its residents or citizens, and thus unmarried females are more likely to adopt modern method of contraception.
The negative realtionship between `ModernC` and the expected number of live births per female (`Fertility`) is also expected by definition.

## Methodology
$~$
### 11. Exercise 9.12 from ALR
Using  $X^TX = X^T_{(i)}X_{(i)} + x_i x_i^T$ where the subscript $(i)$ means without the ith case, show that 

$$
( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}
$$

where $h_{ii}$ is the $i$th diagonal element of $H = X(X^TX)^{-1}X^T$ using direct multiplication and simplify in terms of_ $h_{ii}$.
$~$
$~$
**Solution**:  
We want to show $( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}$.  
Multiply the above equation by $X^T_{(i)}X_{(i)}$ on the LHS and $X^TX - x_i x_i^T$ on the RHS, since $X^TX = X^T_{(i)}X_{(i)} + x_i x_i^T$, the equation we want to prove becomes
$$\begin{aligned}
( X^T_{(i)}X_{(i)})^{-1} * ( X^T_{(i)}X_{(i)}) &=\left((X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}\right)*\left(X^TX - x_i x_i^T\right)\\
&=  (X^TX)^{-1}*\left(X^TX - x_i x_i^T\right) + \frac{(X^TX)^{-1}x_ix_i^T (X^TX)^{-1}}{1 - h_{ii}} *\left(X^TX - x_i x_i^T\right)\\
&= I - (X^TX)^{-1}x_ix_i^T + \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} - \frac{(X^TX)^{-1}x_ix_i^T (X^TX)^{-1}*x_ix_i^T}{1 - h_{ii}}
\end{aligned}$$.  

Since $h_{ii}$ is the $i$th diagonal element of $H = X(X^TX)^{-1}X^T$, we have $h_{ii} = x_i^T(X^TX)^{-1}x_i$, and the equation we want to show can be simplified as

$$\begin{aligned}
I &= I - (X^TX)^{-1}x_ix_i^T + \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} - \frac{(X^TX)^{-1}x_i\left(x_i^T (X^TX)^{-1}x_i\right)x_i^T}{1 - h_{ii}} \\
&= I - (X^TX)^{-1}x_ix_i^T + \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} - \frac{(X^TX)^{-1}x_i*h_{ii}*x_i^T}{1 - h_{ii}}\\
&= I - \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} * \left(1-h_{ii} -1 + h_{ii}\right)\\
& = I
\end{aligned}$$

which is obvious. Thus, $( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}$.



### 12. Exercise 9.13 from ALR.   Using the above, show

$$\hat \beta_{(i)} = \hat\beta -  \frac{(X^TX)^{-1}x_i  e_i}{1 - h_{ii}}$$

**Solution**:  

With case i omitting, we have the OLS estimate for 
$\beta$ as $\hat\beta_{(i)} = (X^T_{(i)}X_{(i)})^{-1} * X_{(i)}^TY_{(i)}$.  
By using the result from Question 11, the estimate can be simplified as the following:
$$\begin{aligned}
\hat\beta_{(i)} & = (X^T_{(i)}X_{(i)})^{-1} * X_{(i)}^TY_{(i)}\\
&=  (X^T_{(i)}X_{(i)})^{-1} * (X^TY - x_iy_i)\\
&= (X^T_{(i)}X_{(i)})^{-1}*X^TY - (X^T_{(i)}X_{(i)})^{-1}*x_iy_i\\
&= (X^TX)^{-1}*X^TY + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}*X^TY}{1 - h_{ii}} - (X^TX)^{-1}x_iy_i + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}x_iy_i}{1 - h_{ii}}\\
& = \hat\beta + \frac{(X^TX)^{-1}x_ix_i^T \hat\beta}{1 - h_{ii}} - \frac{(X^TX)^{-1}(1-h_{ii})x_iy_i}{1-h_{ii}} + \frac{(X^TX)^{-1}x_ih_{ii}y_i}{1 - h_{ii}}\\
&= \hat\beta + \frac{(X^TX)^{-1}x_i\left(x_i^T \hat\beta-y_i\right)}{1 - h_{ii}}\\
&= \hat\beta - \frac{(X^TX)^{-1}x_ie_i}{1 - h_{ii}}
\end{aligned}$$


### 13. (optional)  Prove that the intercept in the added variable scatter plot will always be zero.  

Hint:  use the fact that if $H$ is the projection matrix for $X$ which contains a column of ones, then $1_n^T (I - H) = 0$ or $(I - H) 1_n = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept.








