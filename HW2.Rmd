---
title: "HW2 STA521"
author: 'Jiawei Chen jc762 JC86JC'
date: "Due September 12, 2019 10am"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Background Reading

Readings: Chapters 3-4, 8-9 and Appendix in Weisberg [Applied Linear Regression](https://ebookcentral.proquest.com/lib/duke/reader.action?docID=1574352)  


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This exercise involves the UN data set from `alr3` package. Install `alr3` and the `car` packages and load the data to answer the following questions adding your code in the code chunks.  Please add appropriate code to the chunks to suppress messages and warnings as needed once you are sure the code is working properly and remove instructions if no longer needed. Figures should have informative captions. Please switch the output to pdf for your final version to upload to Sakai. **Remove these instructions for final submission**


## Exploratory Data Analysis

0.  Preliminary read in the data.  After testing, modify the code chunk so that output, messages and warnings are suppressed.  *Exclude text from final*

```{r data, echo = FALSE}
library(alr3)
data(UN3, package="alr3")
library(car)
library(GGally)
library(dplyr)
library(knitr)
```


#### 1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

The summary of the data is shown below. Judging by the no. of NA's in the output, `ModernC`, `Change`, `PPgdp`, `Frate`, `Pop` and `Fertility` in total 6 variables have missing data. Among them, the data-missing problem is particularly serious for `ModernC` and `Frate`, which have 58 and 43 missing values out of 210 respectively.
Based on variable descriptions and their structures, all of these variables should be considered as quantitative.
Other than `Change`, all variables are strictly greater than 0.

```{r}
summary(UN3)
UN3 = na.omit(UN3)
```

$~$
$~$

#### 2. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?

$~$
First of all, from the density plots on the diagonal, we can see that the distributions of `PPgdp` and `Pop` are particularly right skewed, resulting in difficulties in model fitting and interpretations. Besides, the range of these variables is also large. Thus, we should consider transfromation on these two predictors. 

On the other hand, the range for `Change` is rather small, indicating that transformation on it might be of little use.  

In addition, from the first column it seems that `ModernC` has linear relationship with `Change`, `Fertility` and `Purban`, and non-linear relationship with `PPgdp` and `Frate`. Also, it's indicated that most predictors seem to have relationships with each other, and the ones between `Fertility` & `Change` and `Fertility` & `Purban` appear to be linear. 

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=10, fig.cap = 'scatter plot for the UN3 data, untransformed'}
ggpairs(UN3)


```



In the following plot we try to take logs on `Pop` and `PPgdp` to get rid of their large ranges and to inspect pairwise relationships once again. This time, the relationships between `logPPgdp` and many other vairables including `ModernC` seem to be linear, whereas `logPop` still shows no strong relationships with the others. If we are to predict `ModernC` from the other variables, formal tests are needed to decide whether to transfrom or drop these predictors.

```{r message=FALSE, warning=FALSE, fig.width=10, fig.height=10, fig.cap='scatter plot for the UN3 data, take logs on Pop and PPgdp'}
UN0 = UN3 %>%
  mutate(logPop = log(Pop), logPPgdp = log(PPgdp))
ggpairs(UN0, columns = c(1,2,9,4,8,6,7)) 
```



## Model Fitting

#### 3.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?

The summary of the fitted model is shown in the R output below. As we can see, the coefficients for `Change`, `PPgdp`, `Pop`, `Fertility` and the `intercept` seem to be significant with small P-values. The R-squared is 0.6183, indicating that about 61.8% of the variation is explained by our model. From the previous parts, we know that 85 observations are deleted due to missing data, so in total we've used 125 observations in fitting the model. This could also be known from the fact that the df for residual standard error is 118 and we are fitting 7 coefficients, which means 125 observations are used.

```{r}
fmodel = lm(ModernC ~ ., data = UN3)
summary(fmodel)
```

The following plot **Figure 3** gives the diagnostic plots for our linear model. The Residuals vs Fitted plot shows no obvious patterns of the residuals. From the Normal Q-Q plot, there are departures from normality of the standardized residuals, particularly on the upper end, but is not sufficient to render our assumption invalid. The Scale-location plot shows some signs of heteroscedasticity, since the residuals seem to spread the widest when fitted values is around 55, as is indicated by the hump of the red line. The Residuals vs Leverage plot shows no influential point in our model.

```{r fig.width=15, fig.height=12, fig.cap= 'diagnostic plots'}
par(mfrow = c(2,2))
plot(fmodel)

```
$~$
$~$


#### 4. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?  
$~$
The following **Figure 4**  shows the added variable plots for the fitted model. Basically, they indicate the relationship between the response variable `ModernC` and the plotted predictor, after getting rid of the influence from the other predictors.   
Among the six plots, the most unusual one is the added-variable plot for `Pop` predictor. While the majority of the points crowd on the left side of the plot, two observations with row-names `China` and `India` is far away from them. This, once again, proves that `Pop` predictor is particularly in need of transformation due to its overwhelminly strong right skewness. Also, the two observations mentioned, `China` and `India` can be considered influential for `Pop` predictor.  
Apart from that, the plot for `PPgdp` also indicates some need of transfromation, and oberservations `Switzerland` and `Norway` are a bit influential to the fitted line for `PPgdp` predictor.


```{r fig.width=10, fig.height=10, fig.cap='added variable plot'}
avPlots(lm(ModernC ~., data = UN3))
```

#### 5. Using the multivariate BoxCox `car::powerTransform`  find appropriate transformations of the response and predictor variables  for  the linear model.  If any predictors are negative, you may need to transform so that they are non-negative. Summarize the resulting transformations.

In order to find the suitable transformations of our variables, we implement the multivariate BoxCox power transformation on all the response and predictors of our linear model. Notice that since `Change` is not strictly positive, we decide to first transform it into `exp(Change)` to ensure positiveness.  
From the summary shown below, we can see that the suggested $\lambda$ = (1, 0, -0.16, 1, 0, 0, 1) for all the variables. Accompanies with the summary output are two likelihood ratio tests with all $\lambda$ set to 0 and set to 1. As is indicated by the tiny P-values, these two transformations can not lead to a good model.   

Then we try to test whether the suggested $\lambda$ are actually nice powers. The resulted P-value = 0.413 confirms that the transformation is satisfactory.

```{r}
UN = UN3 %>%
  mutate(Change = exp(Change))
mul_transform = car::powerTransform(UN, family = 'bcPower')
str(summary(mul_transform, test))
```


```{r}
testTransform(mul_transform,lambda = c(1,0,-0.16,1,0,0,1))
```


#### 6. Given the selected transformations of the predictors, verify the transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.  Do you get the same transformation if you used `car::powerTransform` above? Do you get the same transformation for the response if you do not transform any of the predictors?  Discuss briefly the findings.
$~$
To verify the transformation acquired in the previous question, in this exercise we use `boxCox` function to transform the response variable once again using the transformed predictors.  
The first plot in **Figure 5** shows the result of the transformation. As is shown, $\lambda = 1$ is well within the 95% confidence interval of the estimated $\lambda$ that maximises the log-likelihood.
If we apply the function to `ModernC` without transformed predictors, we will get the second plot, from which $\lambda = 1$ is hardly included in the 95% CI. The difference in these two results is caused by the fact that without linearly related predictors, 
```{r fig.width=8, fig.height=6, fig.cap='BoxCox transformation of ModernC'}
UN1 = UN %>%
  mutate(Change = log(Change), tran_PPgdp = PPgdp^(-0.16), logPop = log(Pop), logFertility = log(Fertility), PPgdp = NULL, Pop = NULL, Fertility = NULL)

model1 = lm(ModernC~., data = UN1)
boxCox(model1)
boxCox(fmodel)

```

#### 7.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment. If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied with the model and residuals.
$~$

We fit the model with our transformed data. This time, the coefficients of `Frate`, transformed `PPgdp`, transformed `Pop`, transformed `Fertility` and the intercept are significantly different from 0. However, the P-values for `Change` and `Purban` are both large, indicating that these terms have little contributions to our model, and some further ANOVA tests could be considered to see whether to drop them or not.  

**Figure 6** gives the diagnostic plots and **Figure 7** shows the added variable plots for this model. Similar to the original model, in the normal Q-Q plot there are some departures from the normality on both ends, but generally speaking the diagnostic plots show no obvious flaws of the model.  
In the added variable plots, the lines for `Change` and `Purban` are quite flat, indicating that they have no great use in the model. The good news is that no plots have indications of overly influential points this time.

```{r}
transmodel = lm(ModernC ~. , data = UN1)
summary(transmodel)
```



departure from normality
a few outlier?

```{r fig.width=10, fig.height=8, fig.cap='diagnostic plots for the final model'}
par(mfrow = c(2,2))
plot(transmodel)
```


```{r fig.width=10, fig.height=10, fig.cap='added variable plots for the final model}
avPlots(transmodel)
```


#### 8.  Are there any outliers or influential points in the data? Explain. If so, refit the model after removing any outliers/influential points and comment on residual plots.
$~$
By carrying out the outlier test, we can see that there are no outliers or influential points.


```{r}
outlierTest(transmodel)
rownames(UN1)[cooks.distance(transmodel)>0.5]

```

## Summary of Results

#### 9. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient. These should be in terms of the original units! 


```{r}
interval = confint(transmodel)
kable(cbind(transmodel$coefficients, interval), col.names = c('Estimate','2.5%','97.5%'))
```


#### 10. Provide a paragraph summarizing your final model and findings suitable for the US envoy to the UN after adjusting for outliers or influential points. You should provide a justification for any case deletions in your final model.


```{r}

```


## Methodology

    

### 11. Exercise 9.12 from ALR

Using  $X^TX = X^T_{(i)}X_{(i)} + x_i x_i^T$ where the subscript $(i)$ means without the ith case, show that 

$$
( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}
$$

where $h_{ii}$ is the $i$th diagonal element of $H = X(X^TX)^{-1}X^T$ using direct multiplication and simplify in terms of_ $h_{ii}$.

**Solution**:  
We want to show $( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}$.  
Multiply the above equation by $X^T_{(i)}X_{(i)}$ on the LHS and $X^TX - x_i x_i^T$ on the RHS, since $X^TX = X^T_{(i)}X_{(i)} + x_i x_i^T$, the equation we want to prove becomes
$$\begin{aligned}
( X^T_{(i)}X_{(i)})^{-1} * ( X^T_{(i)}X_{(i)}) &=\left((X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}\right)*\left(X^TX - x_i x_i^T\right)\\
&=  (X^TX)^{-1}*\left(X^TX - x_i x_i^T\right) + \frac{(X^TX)^{-1}x_ix_i^T (X^TX)^{-1}}{1 - h_{ii}} *\left(X^TX - x_i x_i^T\right)\\
&= I - (X^TX)^{-1}x_ix_i^T + \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} - \frac{(X^TX)^{-1}x_ix_i^T (X^TX)^{-1}*x_ix_i^T}{1 - h_{ii}}
\end{aligned}$$.  

Since $h_{ii}$ is the $i$th diagonal element of $H = X(X^TX)^{-1}X^T$, we have $h_{ii} = x_i^T(X^TX)^{-1}x_i$, and the equation we want to show can be simplified as

$$\begin{aligned}
I &= I - (X^TX)^{-1}x_ix_i^T + \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} - \frac{(X^TX)^{-1}x_i\left(x_i^T (X^TX)^{-1}x_i\right)x_i^T}{1 - h_{ii}} \\
&= I - (X^TX)^{-1}x_ix_i^T + \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} - \frac{(X^TX)^{-1}x_i*h_{ii}*x_i^T}{1 - h_{ii}}\\
&= I - \frac{(X^TX)^{-1}x_ix_i^T}{1 - h_{ii}} * \left(1-h_{ii} -1 + h_{ii}\right)\\
& = I
\end{aligned}$$

which is obvious. Thus, $( X^T_{(i)}X_{(i)})^{-1} = (X^TX)^{-1} + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}}{1 - h_{ii}}$.



### 12. Exercise 9.13 from ALR.   Using the above, show

$$\hat \beta_{(i)} = \hat\beta -  \frac{(X^TX)^{-1}x_i  e_i}{1 - h_{ii}}$$

**Solution**:  

With case i omitting, we have the OLS estimate for 
$\beta$ as $\hat\beta_{(i)} = (X^T_{(i)}X_{(i)})^{-1} * X_{(i)}^TY_{(i)}$.  
By using the result from Question 11, the estimate can be simplified as the following:
$$\begin{aligned}
\hat\beta_{(i)} & = (X^T_{(i)}X_{(i)})^{-1} * X_{(i)}^TY_{(i)}\\
&=  (X^T_{(i)}X_{(i)})^{-1} * (X^TY - x_iy_i)\\
&= (X^T_{(i)}X_{(i)})^{-1}*X^TY - (X^T_{(i)}X_{(i)})^{-1}*x_iy_i\\
&= (X^TX)^{-1}*X^TY + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}*X^TY}{1 - h_{ii}} - (X^TX)^{-1}x_iy_i + \frac{(X^TX)^{-1}x_ix_i^T  (X^TX)^{-1}x_iy_i}{1 - h_{ii}}\\
& = \hat\beta + \frac{(X^TX)^{-1}x_ix_i^T \hat\beta}{1 - h_{ii}} - \frac{(X^TX)^{-1}(1-h_{ii})x_iy_i}{1-h_{ii}} + \frac{(X^TX)^{-1}x_ih_{ii}y_i}{1 - h_{ii}}\\
&= \hat\beta + \frac{(X^TX)^{-1}x_i\left(x_i^T \hat\beta-y_i\right)}{1 - h_{ii}}\\
&= \hat\beta - \frac{(X^TX)^{-1}x_ie_i}{1 - h_{ii}}
\end{aligned}$$


### 13. (optional)  Prove that the intercept in the added variable scatter plot will always be zero.  

Hint:  use the fact that if $H$ is the projection matrix for $X$ which contains a column of ones, then $1_n^T (I - H) = 0$ or $(I - H) 1_n = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept.








